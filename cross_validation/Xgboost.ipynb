{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반드시 튜닝해야할 파라미터는  min_child_weight / max_depth / gamma\n",
    "\n",
    "xgb.XGBClassifier(\n",
    "    \n",
    "    # General Parameter\n",
    "    booster='gbtree' # 트리,회귀(gblinear) 트리가 항상 디폴트값은 gb트리값임\n",
    "                     # 더 좋은 성능을 내기 때문에 수정할 필요없다고한다.\n",
    "    \n",
    "    silent=True  # running message출력안한다.\n",
    "                 # 모델이 적합되는 과정을 이해하기위해선 False으로한다.\n",
    "    \n",
    "    min_child_weight=10   # 값이 높아지면 under-fitting 되는 \n",
    "                          # 경우가 있다. CV를 통해 튜닝되어야 한다.\n",
    "    \n",
    "    max_depth=8     # 트리의 최대 깊이를 정의함. \n",
    "                    # 루트에서 가장 긴 노드의 거리.\n",
    "                    # 8이면 중요변수에서 결론까지 변수가 9개거친다.\n",
    "                    # Typical Value는 3-10. \n",
    "    \n",
    "    gamma =0    # 노드가 split 되기 위한 loss function의 값이\n",
    "                # 감소하는 최소값을 정의한다. gamma 값이 높아질 수록 \n",
    "                # 알고리즘은 보수적으로 변하고, loss function의 정의\n",
    "                #에 따라 적정값이 달라지기때문에 반드시 튜닝.\n",
    "    \n",
    "    nthread =4    # XGBoost를 실행하기 위한 병렬처리(쓰레드)\n",
    "                  #갯수. 'n_jobs' 를 사용해라.\n",
    "    \n",
    "    colsample_bytree=0.8   # 트리를 생성할때 훈련 데이터에서 \n",
    "                           # 변수를 샘플링해주는 비율. 보통0.6~0.9\n",
    "    \n",
    "    colsample_bylevel=0.9  # 트리의 레벨별로 훈련 데이터의 \n",
    "                           #변수를 샘플링해주는 비율. 보통0.6~0.9\n",
    "    \n",
    "    n_estimators =(int)   #부스트트리의 양\n",
    "                          # 트리의 갯수. \n",
    "    \n",
    "    objective = 'reg:linear','binary:logistic','multi:softmax',\n",
    "                'multi:softprob'  # 4가지 존재.\n",
    "            # 회귀 경우 'reg', binary분류의 경우 'binary',\n",
    "            # 다중분류경우 'multi'- 분류된 class를 return하는 경우 'softmax'\n",
    "            # 각 class에 속할 확률을 return하는 경우 'softprob'\n",
    "    \n",
    "    random_state =  # random number seed.\n",
    "                    # seed 와 동일.\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "XGBClassifier.fit(\n",
    "    \n",
    "    X (array_like)     # Feature matrix ( 독립변수)\n",
    "                       # X_train\n",
    "    \n",
    "    Y (array)          # Labels (종속변수)\n",
    "                       # Y_train\n",
    "    \n",
    "    eval_set           # 빨리 끝나기 위해 검증데이터와 같이써야한다.  \n",
    "                       # =[(X_train,Y_train),(X_vld, Y_vld)]\n",
    " \n",
    "    eval_metric = 'rmse','error','mae','logloss','merror',\n",
    "                'mlogloss','auc'  \n",
    "              # validation set (검증데이터)에 적용되는 모델 선택 기준.\n",
    "              # 평가측정. \n",
    "              # 회귀 경우 rmse ,  분류 -error   이외의 옵션은 함수정의\n",
    "    \n",
    "    early_stopping_rounds=100,20\n",
    "              # 100번,20번 반복동안 최대화 되지 않으면 stop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
